{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "model .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codezero-01/RecipeGen/blob/main/model_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neural-treat",
        "outputId": "0f2f373a-78bf-4705-c54c-80125ae2fe04"
      },
      "source": [
        "# Import statements\n",
        "import math\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "import h5py\n",
        "import torch\n",
        "\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelWithLMHead,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    PreTrainedTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    set_seed,\n",
        ")\n",
        "\n",
        "#  select the GPU to train the model on\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
        "# --optional (to debug the cuda error)\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\""
      ],
      "id": "neural-treat",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch version: 1.8.1+cu102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "analyzed-africa"
      },
      "source": [
        "# To inspect cuda memory allotment and clearing cuda cache\n",
        "# print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
        "# torch.cuda.empty_cache()\n"
      ],
      "id": "analyzed-africa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "economic-sight"
      },
      "source": [
        ""
      ],
      "id": "economic-sight",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "supposed-stable"
      },
      "source": [
        "# sample training instance \n",
        "samples_data = []\n",
        "with open('train_temp.txt') as file:\n",
        "    for row in file:\n",
        "        samples_data.append(row)"
      ],
      "id": "supposed-stable",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "valuable-holder",
        "outputId": "ed1e610c-1aaa-4458-bd08-168c36ec0ac8"
      },
      "source": [
        "samples_data[0]"
      ],
      "id": "valuable-holder",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<BEGIN_RECIPE> <BEGIN_INPUT> baking <NEXT_INPUT> chili_powder <NEXT_INPUT> chili <NEXT_INPUT> oven <NEXT_INPUT> baked <END_INPUT> <BEGIN_TITLE> Indian Spiced Baked Potato Sticks<END_TITLE> <BEGIN_INGREDS> potato <NEXT_INGREDS> kitchen <NEXT_INGREDS> ghee <NEXT_INGREDS> butter <NEXT_INGREDS> oil <NEXT_INGREDS> turmeric <NEXT_INGREDS> chili_powder <NEXT_INGREDS> curry <NEXT_INGREDS> leaves <NEXT_INGREDS> salt <NEXT_INGREDS> parchment_paper <NEXT_INGREDS> chips <END_INGREDS> <BEGIN_INSTR> pre heat oven to 400°f . wash and dry the potato . peel the potato if you like . i leave the skin on because i like the texture and nutrients . slice the potato into 1/8th inch thick slices lengthwise using a mandolin or a large , sharp knife . stack up the slices , and cut them lengthwise to form thin sticks . place the potato sticks on a kitchen towel , and gently pat dry . in a bowl , whisk together the ghee or melted butter or oil , turmeric , chili powder , curry leaves and salt . add the potato sticks to the bowl and toss well . place the seasoned potato sticks on a non stick baking sheet or a regular baking sheet lined with parchment paper or silpat . spread them they are in a single layer and don’t overlap . bake for 10 minutes or till the chips are golden and crisp . halfway through baking , check on the chips , and remove any pieces that are already crispy and golden . rotate the baking sheet and continue baking until done . <END_INSTR> <END_RECIPE>\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "parliamentary-camcorder"
      },
      "source": [
        "#  Model parameters and hyper-parameters\n",
        "\n"
      ],
      "id": "parliamentary-camcorder",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "painful-medication"
      },
      "source": [
        "# PyTorch dataset class \n",
        "class H5Dataset(Dataset):\n",
        "    def __init__(self, tokenizer, file_path='train_temp', block_size=512): \n",
        "        cached_features_file = \"data_temp.h5\"\n",
        "\n",
        "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "        print((\"Loading features from cached file %s\", cached_features_file))\n",
        "        with h5py.File(cached_features_file, 'r') as f:\n",
        "            if file_path=='test_temp':\n",
        "                self.samples = f[file_path][:] #this is a dev set, 30% of a test set\n",
        "            else:\n",
        "                self.samples = f[file_path][:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return torch.tensor(self.samples[item]) \n"
      ],
      "id": "painful-medication",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voluntary-crossing"
      },
      "source": [
        "def get_dataset( tokenizer, evaluate=False, local_rank=-1):\n",
        "    file_path = \"test_temp\" if evaluate else \"train_temp\"\n",
        "    return H5Dataset(tokenizer=tokenizer, file_path=file_path)"
      ],
      "id": "voluntary-crossing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amino-perry"
      },
      "source": [
        "# set global seed \n",
        "set_seed(20)"
      ],
      "id": "amino-perry",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naval-operator",
        "colab": {
          "referenced_widgets": [
            "4e6f920c66784f3d944070807172effe"
          ]
        },
        "outputId": "fc83e00c-7ab0-4b8b-99d9-4271595f7e26"
      },
      "source": [
        "# Transformer configuration\n",
        "config = AutoConfig.from_pretrained('gpt2', cache_dir='cache')"
      ],
      "id": "naval-operator",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e6f920c66784f3d944070807172effe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "independent-method",
        "colab": {
          "referenced_widgets": [
            "7717b5cce1ce4e78a6319f15ea26fbbf",
            "a02c6aae14f44460bf702a363eca2dc1",
            "aad5eecc374b47529c956a9e8e09d604"
          ]
        },
        "outputId": "6dee3e89-f120-4ef5-b8b9-1005186105fa"
      },
      "source": [
        "# Tokenizer for the model training\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2', cache_dir= 'cache')"
      ],
      "id": "independent-method",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7717b5cce1ce4e78a6319f15ea26fbbf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a02c6aae14f44460bf702a363eca2dc1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aad5eecc374b47529c956a9e8e09d604",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "favorite-egypt",
        "colab": {
          "referenced_widgets": [
            "32a073d01584408286cfca6ec2cf00a0"
          ]
        },
        "outputId": "0db8b849-94c6-47e6-fcaf-d0468004553f"
      },
      "source": [
        "# Initialising the GPT2 model\n",
        "model = AutoModelWithLMHead.from_pretrained(\n",
        "            'gpt2', # model name\n",
        "            config=config,\n",
        "            cache_dir='cache', # cache directory (path to the cache directory)\n",
        "        )"
      ],
      "id": "favorite-egypt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/vijay20052/anaconda3/envs/deepgpu/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:1006: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  warnings.warn(\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "32a073d01584408286cfca6ec2cf00a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "provincial-reception",
        "outputId": "98fc01ef-b6ff-41b4-ac0d-a89fb08d6cba"
      },
      "source": [
        "# Adding the sppecial recipe token to the tokenizer\n",
        "special_tokens = {\n",
        "    \"additional_special_tokens\": [\"<BEGIN_RECIPE>\" ,\n",
        "                                \"<BEGIN_INPUT>\" , \n",
        "                                \"<NEXT_INPUT>\",\n",
        "                                \"<END_INPUT>\" ,\n",
        "                                \"<BEGIN_TITLE>\",\n",
        "                                \"<END_TITLE>\" ,\n",
        "                                \"<BEGIN_INGREDS>\" ,\n",
        "                                \"<NEXT_INGREDS>\" ,\n",
        "                                \"<END_INGREDS>\" ,\n",
        "                                \"<BEGIN_INSTR>\" ,\n",
        "                                \"<NEXT_INSTR>\",\n",
        "                                \"<END_INSTR>\" ,\n",
        "                                \"<END_RECIPE>\"  \n",
        "    ]\n",
        "}\n",
        "\n",
        "\n",
        "tokenizer.add_special_tokens(special_tokens)\n",
        "model.resize_token_embeddings(len(tokenizer))  # resizeing the model to fit the tokenizer with special tokens"
      ],
      "id": "provincial-reception",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50270, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "occasional-island",
        "outputId": "eb14f36e-db9d-4d41-dba0-158b82cdfbf4"
      },
      "source": [
        "tokenizer, len(tokenizer) # verifying the toknizer configurations and the length of tokenizer"
      ],
      "id": "occasional-island",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PreTrainedTokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<BEGIN_RECIPE>', '<BEGIN_INPUT>', '<NEXT_INPUT>', '<END_INPUT>', '<BEGIN_TITLE>', '<END_TITLE>', '<BEGIN_INGREDS>', '<NEXT_INGREDS>', '<END_INGREDS>', '<BEGIN_INSTR>', '<NEXT_INSTR>', '<END_INSTR>', '<END_RECIPE>']}),\n",
              " 50270)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrong-chuck"
      },
      "source": [
        ""
      ],
      "id": "wrong-chuck",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "solid-presence"
      },
      "source": [
        ""
      ],
      "id": "solid-presence",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taken-industry",
        "outputId": "fc33f313-6003-4b2f-bfb9-1022b59df788"
      },
      "source": [
        "# converting the train and validation dataset to pytorch Dataset so as it can be given to the model as input for training\n",
        "train_dataset = ( get_dataset(tokenizer=tokenizer) )\n",
        "eval_dataset = (  get_dataset(tokenizer=tokenizer, evaluate=True) )"
      ],
      "id": "taken-industry",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Loading features from cached file %s', 'data_temp.h5')\n",
            "('Loading features from cached file %s', 'data_temp.h5')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "everyday-nomination"
      },
      "source": [
        "'''\n",
        "* To be able to build batches, data collators may apply some processing (like padding). Some of them\n",
        "(like DataCollatorForLanguageModeling) also apply some random data augmentation (like random masking) oin the formed batch.\n",
        "* Data collators are objects that will form a batch by using a list of dataset elements as input. These elements are of the same type as the elements of train_dataset or eval_dataset.\n",
        "* Forming the batches to dataset to be trained\n",
        "source :- Hugginface.co\n",
        "'''\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False, mlm_probability=0.15  )"
      ],
      "id": "everyday-nomination",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "accessible-briefs"
      },
      "source": [
        "training_args = TrainingArguments(\n",
        "    \n",
        "    output_dir= \"./outputs/tempt\",\n",
        "    \n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=8,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    fp16_opt_level='O1',\n",
        "    warmup_steps=1e2,    \n",
        "    learning_rate=5e-4,\n",
        "    adam_epsilon=1e-8,\n",
        "    weight_decay=0.01,        \n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=True,     \n",
        ")"
      ],
      "id": "accessible-briefs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adopted-survey"
      },
      "source": [
        ""
      ],
      "id": "adopted-survey",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "occupied-clearance"
      },
      "source": [
        "# Initializing our PyTorch Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        ")"
      ],
      "id": "occupied-clearance",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seventh-vulnerability",
        "outputId": "db958f39-05a5-4dc6-d6b4-15762e0893f3"
      },
      "source": [
        "# saving the tokenizer object \n",
        "tokenizer.save_pretrained('./outputs/tempt/')"
      ],
      "id": "seventh-vulnerability",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./outputs/tempt1/tokenizer_config.json',\n",
              " './outputs/tempt1/special_tokens_map.json',\n",
              " './outputs/tempt1/vocab.json',\n",
              " './outputs/tempt1/merges.txt',\n",
              " './outputs/tempt1/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "political-actor"
      },
      "source": [
        ""
      ],
      "id": "political-actor",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "verbal-sigma"
      },
      "source": [
        ""
      ],
      "id": "verbal-sigma",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diagnostic-allergy",
        "outputId": "8e0c9a15-79ee-41dd-8308-fc72b2653716"
      },
      "source": [
        "# Starting the Training and saving the model\n",
        "trainer.train()\n",
        "trainer.save_model() "
      ],
      "id": "diagnostic-allergy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/vijay20052/anaconda3/envs/deepgpu/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='1480' max='1480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1480/1480 5:10:38, Epoch 4/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Runtime</th>\n",
              "      <th>Samples Per Second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.581332</td>\n",
              "      <td>64.770700</td>\n",
              "      <td>9.464000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.902000</td>\n",
              "      <td>1.484565</td>\n",
              "      <td>63.638600</td>\n",
              "      <td>9.633000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.902000</td>\n",
              "      <td>1.443501</td>\n",
              "      <td>196.873300</td>\n",
              "      <td>3.114000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.461600</td>\n",
              "      <td>1.420606</td>\n",
              "      <td>192.589700</td>\n",
              "      <td>3.183000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.461600</td>\n",
              "      <td>1.411004</td>\n",
              "      <td>182.421900</td>\n",
              "      <td>3.360000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "italic-literacy"
      },
      "source": [
        ""
      ],
      "id": "italic-literacy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "linear-integrity",
        "outputId": "9ef18c15-a3c9-4267-ba87-9629eb87ed3f"
      },
      "source": [
        "# !nvidia-smi"
      ],
      "id": "linear-integrity",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon May  3 13:48:00 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.100      Driver Version: 440.100      CUDA Version: 10.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-PCIE...  Off  | 00000000:5E:00.0 Off |                    0 |\n",
            "| N/A   41C    P0    36W / 250W |  31025MiB / 32510MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   1  Tesla V100-PCIE...  Off  | 00000000:D8:00.0 Off |                    0 |\n",
            "| N/A   47C    P0    38W / 250W |   8001MiB / 32510MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|    0     23057      C   python                                     31013MiB |\n",
            "|    1     18995      C   ...e/koushikb/.conda/envs/tfgpu/bin/python  7989MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "entitled-sheep"
      },
      "source": [
        ""
      ],
      "id": "entitled-sheep",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scheduled-jewelry",
        "outputId": "f493c311-4430-4d40-d619-1b793249d966"
      },
      "source": [
        "# saving the tokenizer after training the mode, just to be safe\n",
        "tokenizer.save_pretrained('./outputs/tempt/')"
      ],
      "id": "scheduled-jewelry",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./outputs/tempt1/tokenizer_config.json',\n",
              " './outputs/tempt1/special_tokens_map.json',\n",
              " './outputs/tempt1/vocab.json',\n",
              " './outputs/tempt1/merges.txt',\n",
              " './outputs/tempt1/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aging-instrumentation"
      },
      "source": [
        ""
      ],
      "id": "aging-instrumentation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "united-essex",
        "outputId": "ea94f75e-895a-4107-f495-e6ce4c6a9408"
      },
      "source": [
        "results = {}\n",
        "    \n",
        "eval_output = trainer.evaluate()\n",
        "\n",
        "# perplixity in case of pytorch can be calculated using below computation\n",
        "perplexity = math.exp(eval_output[\"eval_loss\"])\n",
        "result = {\"perplexity\": perplexity}"
      ],
      "id": "united-essex",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA out of memory. Tried to allocate 786.00 MiB (GPU 0; 31.75 GiB total capacity; 4.33 GiB already allocated; 469.19 MiB free; 4.65 GiB reserved in total by PyTorch)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-68c0f65cde8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0meval_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mperplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/deepgpu/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   1665\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1667\u001b[0;31m         output = self.prediction_loop(\n\u001b[0m\u001b[1;32m   1668\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1669\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/deepgpu/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mprediction_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   1803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1805\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_loss_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1806\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1807\u001b[0m                 \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/deepgpu/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mprediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   1917\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhas_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1919\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1920\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1921\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/deepgpu/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1473\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1475\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1476\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1477\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/deepgpu/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/deepgpu/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;31m# Flatten the tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshift_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/deepgpu/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/deepgpu/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1048\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/deepgpu/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2691\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2693\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/deepgpu/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1670\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"log_softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1672\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1673\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 786.00 MiB (GPU 0; 31.75 GiB total capacity; 4.33 GiB already allocated; 469.19 MiB free; 4.65 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adapted-reputation",
        "outputId": "d4f668a2-299f-41c2-b528-0c77a5bc0626"
      },
      "source": [
        "print(eval_output, '\\n\\n', result)"
      ],
      "id": "adapted-reputation",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'eval_loss': 1.4110041856765747, 'eval_runtime': 182.9562, 'eval_samples_per_second': 3.351, 'epoch': 5.0, 'eval_mem_cpu_alloc_delta': 55255, 'eval_mem_gpu_alloc_delta': 0, 'eval_mem_cpu_peaked_delta': 25460, 'eval_mem_gpu_peaked_delta': 2935491072} \n",
            "\n",
            " {'perplexity': 4.100070569669201}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fabulous-clearance"
      },
      "source": [
        ""
      ],
      "id": "fabulous-clearance",
      "execution_count": null,
      "outputs": []
    }
  ]
}