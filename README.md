# RecipeGen
Novel recipe generation and evaluation using GPT2

#### * GPT2 small is used having 768 tokens. 
#### * packages used hugginface transformers, PyTorch and Sklearn.


# Future works

* Training the model on the complete dataset. 
* Extracting more and better keywords from the recipes.
* Trying the GPT2-medium model ( a bigger variant of gpt2 ).
* Trying gpt-neo ( open sources version on gpt3).
* Preprocessing the dataset such that quantities of the ingredients are also included. 
* Separating the individual cooking instructions.
* Trying out merging the RecipeDB dataset with some more recipe data (recipe 1m, recipenlg etc).
* Fine-Tuning the hyperparameters of the model.  

